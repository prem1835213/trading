## Goal:
- Evaluate the current state in terms of state-value or action-value

## Questions

- How do state-value functions (v(s)) or state-action-value (q(s, a)) get evaluated in practice?
-- Is sampling only done during offline training?
-- How can we sample if we don't know the next state?

## To-do:

- Create a training set
- Understand the labels needed for prediction

## Prerequisites
- Allow interaction with the environment
- Define a state
- Define actions
- Define rewards
